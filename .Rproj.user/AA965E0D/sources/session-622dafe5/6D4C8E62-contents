---
pagetitle: "Stat1"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float:
     collapsed: false
     smooth_scroll: false
---

<link rel="stylesheet" href="styles.css" type="text/css">
<link rel="stylesheet" href="site_libs/academicons-1.9.1/css/academicons.min.css"/>

<br><br><br>

## **Private Library Customer Segmentation and Lifetime Value Prediction**

![](https://img.shields.io/badge/Using-Python-blue)
![](https://img.shields.io/badge/-RandomForest-red)
![](https://img.shields.io/badge/-KmeansClustering-success)

<br>



<br><br><br>

### 1. Figure

<p align="center">
<img src="images/silhouette_score_comparison.png" style="width:80%; border:0px solid; margin-right: 20px" align="center">
</p>
<p align="center">
[Fig. LASSO regression features by importance]
</p>


<br>

<br><br>

### 2. Goal
The analysis aims to segment library users based on behavior and engagement while predicting customer lifetime value (CLV) to identify valuable user groups and guide strategies for retention and revenue growth.

<br>

### 3. Methodology & Summary

  + LASSO regressions showed the best performance with a cross validation RMSE-score of 0.1121. Although there is a lot of multicollinearity among the variables, LASSO regression include feature selection; it does not select a substantial number of the available variables in its model, as it is supposed to do.
  + The XGBoost model also performs very well with a cross validation RMSE of 0.1162.
  + As those two algorithms are very different, averaging predictions is likely to improve the predictions. As the Lasso cross validated RMSE is better than XGBoost's CV score, I decided to weight the Lasso results double.

<br>

### 4. Code

Please click [HERE](https://nbviewer.org/github/seryeongkyun/seryeongkyun.github.io/blob/main/files/Library_Customer_Segmentation_and_Lifetime_Value_Prediction.ipynb) for the analysis report and code.

<br>


